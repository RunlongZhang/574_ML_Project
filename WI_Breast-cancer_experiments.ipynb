{"cells":[{"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nimport numpy\nimport matplotlib.pyplot\nimport numpy.linalg \nimport numpy.random\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom sklearn.model_selection import KFold\nfrom mpl_toolkits import mplot3d\nfrom sklearn.feature_selection import mutual_info_classif\nfrom skfeature.function.similarity_based import fisher_score\n\n\n\ndef parseData(filename):\n    f = open(filename, \"r\")\n    line = f.readline()\n    X = numpy.zeros((0, 30))\n    y = []\n    while line:\n        line = line.strip()\n        a = line.split(\",\")\n\n        if (a[1] == \"M\"):\n            y.append(1)\n        else:\n            y.append(0)\n\n        del a[0:2]\n        b = [eval(i) for i in a]\n        X = numpy.vstack((X, b))\n        line = f.readline()\n    f.close()\n    return X, y\n\ndef deletepng():\n    folder_path = (r'/work/')\n    test = os.listdir(folder_path)\n    for images in test:\n        if images.endswith(\".png\"):\n            os.remove(os.path.join(folder_path, images))\n\ndef sigmoid(x):\n    return 1.0/(1 + numpy.exp(-x))\n\ndef loss(y, prediction):\n    loss = -numpy.mean(y * (numpy.log(prediction)) - (1 - y) * numpy.log(1 - prediction))\n    return loss\n\ndef gradient(X, y, predictions):\n    m = X.shape[0]\n    dw = (1/m) * numpy.dot(X.T, (predictions - y))\n    db = (1/m) * numpy.sum((predictions - y))\n    return dw, db\n\ndef plot_2d_boundary(X, y, w, b, filename):\n    x_ticks = [min(X[:, 0]), max(X[:, 0])]\n    m = -w[0]/w[1]\n    c = -b/w[1]\n    y_ticks = m * x_ticks + c\n\n    C1 = numpy.zeros((0, 2))\n    C2 = numpy.zeros((0, 2))\n\n    for i in range(len(y)):\n        if y[i] == 1:\n            C1 = numpy.vstack((C1, X[i]))\n        else:\n            C2 = numpy.vstack((C2, X[i]))\n\n    matplotlib.pyplot.xlim(numpy.min(X[:,0]), numpy.max(X[:,0]))\n    matplotlib.pyplot.ylim(numpy.min(X[:,1]), numpy.max(X[:,1]))\n    matplotlib.pyplot.plot(C1[:, 0], C1[:, 1], 'ro')\n    matplotlib.pyplot.plot(C2[:, 0], C2[:, 1], 'bo')\n    matplotlib.pyplot.plot(x_ticks, y_ticks, 'k-')\n    matplotlib.pyplot.savefig(filename)\n    matplotlib.pyplot.close('all')\n\ndef plot_3d_boundary(X, y, w, b, filename):\n    C1 = numpy.zeros((0, 3))\n    C2 = numpy.zeros((0, 3))\n\n    for i in range(len(y)):\n        if y[i] == 1:\n            C1 = numpy.vstack((C1, X[i]))\n        else:\n            C2 = numpy.vstack((C2, X[i]))\n    fig = plt.figure(figsize = (10,10))\n    ax = plt.axes(projection='3d')\n    #x1_vals = np.linspace(10, 25, 50)\n    #x2_vals = np.linspace(10, 30, 50)\n    x1_vals = np.linspace(numpy.min(X[:, 0]), numpy.max(X[:, 0]), 50)\n    x2_vals = np.linspace(numpy.min(X[:, 1]), numpy.max(X[:, 1]), 50)\n    x1_mesh, x2_mesh = np.meshgrid(x1_vals, x2_vals)\n    x3_mesh = (-b - w[0]*x1_mesh - w[1]*x2_mesh) / w[2]\n    ax.scatter(C1[:, [0]], C1[:, [1]], C1[:, [2]], c = 'r', s = 15)\n    ax.scatter(C2[:, [0]], C2[:, [1]], C2[:, [2]], c = 'b', s = 15)\n    ax.plot_surface(x1_mesh, x2_mesh, x3_mesh, alpha=0.5)\n    #ax.view_init(0, -100)\n    ax.view_init(10, 20)\n    matplotlib.pyplot.savefig(filename)\n    matplotlib.pyplot.close('all')\n\ndef plot_data_2d(X, y, filename):\n    C1 = numpy.zeros((0, 2))\n    C2 = numpy.zeros((0, 2))\n\n    for i in range(len(y)):\n        if y[i] == 1:\n            C1 = numpy.vstack((C1, X[i]))\n        else:\n            C2 = numpy.vstack((C2, X[i]))\n\n    matplotlib.pyplot.xlim(numpy.min(X[:,0]), numpy.max(X[:,0]))\n    matplotlib.pyplot.ylim(numpy.min(X[:,1]), numpy.max(X[:,1]))\n    matplotlib.pyplot.plot(C1[:, 0], C1[:, 1], 'ro')\n    matplotlib.pyplot.plot(C2[:, 0], C2[:, 1], 'bo')\n    matplotlib.pyplot.savefig(filename)\n    matplotlib.pyplot.close('all')\n\n\ndef plot_data_3d(X, y, filename):\n    C1 = numpy.zeros((0, 3))\n    C2 = numpy.zeros((0, 3))\n\n    for i in range(len(y)):\n        if y[i] == 1:\n            C1 = numpy.vstack((C1, X[i]))\n        else:\n            C2 = numpy.vstack((C2, X[i]))\n    fig = plt.figure(figsize = (10,10))\n    ax = plt.axes(projection='3d')\n    ax.scatter(C1[:, [0]], C1[:, [1]], C1[:, [2]], c = 'r', s = 15)\n    ax.scatter(C2[:, [0]], C2[:, [1]], C2[:, [2]], c = 'b', s = 15)\n    matplotlib.pyplot.savefig(filename)\n    matplotlib.pyplot.close('all')\n\ndef normalize(X):\n    m, n = X.shape\n    for i in range(n):\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    return X\n\ndef train(X, y, batchSize, learningRate, iterations):\n    m, n = X.shape\n    w = numpy.zeros((n, 1))\n    b = 0\n\n    y = y.reshape(m, 1)\n\n    x = normalize(X)\n\n    losses = []\n\n    for i in range(iterations):\n        for j in range((m-1) // batchSize + 1):\n            start = j * batchSize\n            end = start + batchSize\n            x_batch = X[start:end]\n            y_batch = y[start:end]\n\n            predictions = sigmoid(numpy.dot(x_batch, w) + b)\n\n            dw, db = gradient(x_batch, y_batch, predictions)\n\n            w -= learningRate * dw\n            b -= learningRate * db\n\n        #l = loss(y, sigmoid(numpy.dot(X, w) + b))\n        #losses.append(l)\n    \n    return w, b#, losses\n\ndef predict(X, w, b):\n    x = normalize(X)\n\n    predictions = sigmoid(numpy.dot(X, w) + b)\n\n    prediction_class = []\n    prediction_class = [1 if i > 0.5 else 0 for i in predictions]\n\n    return numpy.array(prediction_class)\n\ndef accuracy(y, predictions):\n    accuracy = numpy.sum(y == predictions) / len(y)\n    return accuracy\n\ndef CV_kfold(X, y, batchSize, learningRate, iterations, k):\n    kf = KFold(n_splits=k)\n    atrains = []\n    atests = []\n    awholes = []\n    wks = []\n    bks = []\n\n    for i, (train_i, test_i) in enumerate(kf.split(Xs)):\n        train_X, test_X = X[train_i], X[test_i]\n        train_y, test_y = y[train_i], y[test_i]\n        wk, bk = train(train_X, train_y, batchSize, learningRate, iterations)\n\n        predictions_train = predict(train_X, wk, bk)\n        predictions_test = predict(test_X, wk, bk)\n        predictions_whole = predict(X, wk, bk)\n\n        accuracy_train = accuracy(train_y, predictions_train)\n        accuracy_test = accuracy(test_y, predictions_test)\n        accuracy_whole = accuracy(y, predictions_whole)\n\n        atrains.append(accuracy_train)\n        atests.append(accuracy_test)\n        awholes.append(accuracy_whole)\n\n        wks.append(wk)\n        bks.append(bk)\n    \n    return atrains, atests, awholes, wks, bks\n\ndef print_kfold_accuracy(atrains, atests, awholes):\n    for i in range(len(atrains)):\n        print(\"Fold: \", i + 1)\n        print(\"Training accuracy: \", atrains[i])\n        print(\"Testing accuracy: \", atests[i])\n        print(\"Overall accuracy: \", awholes[i])\n\ndef information_gain(X, y, color, filename):\n    importances = mutual_info_classif(X, y)\n    feat_importances = pd.Series(importances)\n    feat_importances.plot(kind = \"barh\", color = color)\n    plt.title(\"Information Gain\")\n    plt.savefig(filename)\n    plt.close('all')\n    return importances\n\ndef fishers_score(X, y, color, filename):\n    ranks = fisher_score.fisher_score(X, y)\n    feat_importances = pd.Series(ranks)\n    feat_importances.plot(kind = \"barh\", color = color)\n    plt.title(\"Fisher Score\")\n    plt.savefig(filename)\n    plt.close('all')\n    return ranks\n\ndef correlation_coefficient(X, y, filename):\n    dataframe = pd.DataFrame(X)\n    correlation = dataframe.corr()\n    plt.figure(figsize = (30, 20))\n    sns.heatmap(correlation, annot = True)\n    plt.title(\"Correlation Matrix\")\n    plt.savefig(filename)\n    plt.close('all')\n    return correlation\n\ndef dispersion_ratio(X, color, filename):\n    X = X + 1\n    am = numpy.mean(X, axis = 0)\n    gm = numpy.power(numpy.prod(X, axis = 0), 1 / X.shape[0])\n    ratio = am / gm\n    ratios = pd.Series(ratio)\n    ratios.plot(kind = \"barh\", color = color)\n    plt.title(\"Dispersion Ratio\")\n    plt.savefig(filename)\n    plt.close('all')\n    return ratio\n\ndef superimpose_bar(importances, ranks, color1, color2, filename):\n    info_importances = pd.Series(importances)\n    info_importances = info_importances * 60\n    fisher_importances = pd.Series(ranks)\n    info_importances.plot(kind = \"barh\", color = color1)\n    fisher_importances.plot(kind = \"barh\", color = color2, alpha = 0.5)\n    plt.title(\"Super-imposed feature importances\")\n    plt.savefig(filename)\n    plt.close('all')\n\ndef weighted_sum_importance(importances, ranks, color, filename):\n    info_importances = pd.Series(importances)\n    info_importances = info_importances * 60\n    fisher_importances = pd.Series(ranks)\n    weighted_sum = info_importances + fisher_importances\n    weighted_sum.plot(kind = \"barh\", color = color)\n    plt.title(\"Weighted sum importances\")\n    plt.savefig(filename)\n    plt.close('all')\n\ndef grid_best_performance(X, y, batchSize, learningRate, iterations, folds):\n    best_overall_accs = []\n    best_testing_accs = []\n    best_training_accs = []\n    lr = learningRate * 100\n    for i in range(5, batchSize, 5):\n        for j in range(1, int(lr), 1):\n            for m in range(250, iterations, 250):\n                for n in range(2, folds, 1):\n                    atrains, atests, awholes, wks, bks = CV_kfold(X, y, i, j/100, m, n)\n                    best_overall_accs.append(max(awholes))\n                    best_testing_accs.append(max(atests))\n                    best_training_accs.append(max(atrains))\n    boa = max(best_overall_accs)\n    btesta = max(best_testing_accs)\n    btraina = max(best_training_accs)\n    return btraina, btesta, boa\n\ndef grid_best_performance_bs(X, y, batchSize, learningRate, iterations, folds):\n    best_overall_accs = []\n    best_testing_accs = []\n    best_training_accs = []\n    for i in range(5, batchSize, 5):\n        atrains, atests, awholes, wks, bks = CV_kfold(X, y, i, learningRate, iterations, folds)\n        best_overall_accs.append(max(awholes))\n        best_testing_accs.append(max(atests))\n        best_training_accs.append(max(atrains))\n    boa = max(best_overall_accs)\n    btesta = max(best_testing_accs)\n    btraina = max(best_training_accs)\n    boai = best_overall_accs.index(boa) * 5 + 5\n    btestai = best_testing_accs.index(btesta) * 5 + 5\n    btrainai = best_training_accs.index(btraina) * 5 + 5\n    return btraina, btesta, boa, btrainai, btestai, boai\n\ndef grid_best_performance_lr(X, y, batchSize, learningRate, iterations, folds):\n    best_overall_accs = []\n    best_testing_accs = []\n    best_training_accs = []\n    lr = learningRate * 100\n    for i in range(1, int(lr), 1):\n        atrains, atests, awholes, wks, bks = CV_kfold(X, y, batchSize, i/100, iterations, folds)\n        best_overall_accs.append(max(awholes))\n        best_testing_accs.append(max(atests))\n        best_training_accs.append(max(atrains))\n    boa = max(best_overall_accs)\n    btesta = max(best_testing_accs)\n    btraina = max(best_training_accs)\n    boai = best_overall_accs.index(boa) / 100 + 0.01\n    btestai = best_testing_accs.index(btesta) / 100 + 0.01\n    btrainai = best_training_accs.index(btraina) / 100 + 0.01\n    return btraina, btesta, boa, btrainai, btestai, boai\n\ndef grid_best_performance_it(X, y, batchSize, learningRate, iterations, folds):\n    best_overall_accs = []\n    best_testing_accs = []\n    best_training_accs = []\n    for i in range(50, iterations, 50):\n        atrains, atests, awholes, wks, bks = CV_kfold(X, y, batchSize, learningRate, i, folds)\n        best_overall_accs.append(max(awholes))\n        best_testing_accs.append(max(atests))\n        best_training_accs.append(max(atrains))\n    boa = max(best_overall_accs)\n    btesta = max(best_testing_accs)\n    btraina = max(best_training_accs)\n    boai = best_overall_accs.index(boa) * 50 + 50\n    btestai = best_testing_accs.index(btesta) * 50 + 50\n    btrainai = best_training_accs.index(btraina) * 50 + 50\n    return btraina, btesta, boa, btrainai, btestai, boai\n\ndef grid_best_performance_k(X, y, batchSize, learningRate, iterations, folds):\n    best_overall_accs = []\n    best_testing_accs = []\n    best_training_accs = []\n    for i in range(2, folds, 1):\n        atrains, atests, awholes, wks, bks = CV_kfold(X, y, batchSize, learningRate, iterations, i)\n        best_overall_accs.append(max(awholes))\n        best_testing_accs.append(max(atests))\n        best_training_accs.append(max(atrains))\n    boa = max(best_overall_accs)\n    btesta = max(best_testing_accs)\n    btraina = max(best_training_accs)\n    boai = best_overall_accs.index(boa) + 2\n    btestai = best_testing_accs.index(btesta) + 2\n    btrainai = best_training_accs.index(btraina) + 2\n    return btraina, btesta, boa, btrainai, btestai, boai\n\ndef objective(params):\n    batchSize, learningRate, iterations = params\n    w, b, l = train(Xs, y, batchSize, learningRate, iterations)\n    predictions = predict(val_x, w, b)\n    val_acc = accuracy(val_y, predictions)\n    return -val_acc\n\ndef objective4D(params):\n    batchSize, learningRate, iterations, k = params\n    atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, batchSize, learningRate, iterations, k)\n    accs = []\n    for i in range(len(wks)):\n        predictions = predict(val_x, wks[i], bks[i])\n        val_acc = accuracy(val_y, predictions)\n        accs.append(val_acc)\n    return -numpy.mean(accs)\n\ndef printCVtestAccuracy(test_x, test_y, wks, bks):\n    for i in range(len(wks)):\n        predictions = predict(test_x, wks[i], bks[i])\n        acc = accuracy(test_y, predictions)\n        print(\"Fold: \", i + 1)\n        print(\"Testing Accuracy: \", acc)\n\ndef printCVAllAccuracy(X, y, val_x, val_y, test_x, test_y, wks, bks):\n    for i in range(len(wks)):\n        predictions = predict(X, wks[i], bks[i])\n        trainAcc = accuracy(y, predictions)\n        predictions = predict(val_x, wks[i], bks[i])\n        valAcc = accuracy(val_y, predictions)\n        predictions = predict(test_x, wks[i], bks[i])\n        testAcc = accuracy(test_y, predictions)\n        print(\"Fold: \", i + 1)\n        print(\"Training Accuracy: \", trainAcc)\n        print(\"Validation Accuracy: \", valAcc)\n        print(\"Testing Accuracy: \", testAcc)\n    \ndef printAccuracy(X, y, w, b):\n    predictions = predict(X, w, b)\n    acc = accuracy(y, predictions)\n    print(\"Accuracy: \", acc)\n\ndef splitData(X, y, trainSplit, valSplit, testSplit):\n    trainStop = int(trainSplit * len(X))\n    valStop = int((trainSplit + valSplit) * len(X))\n    train_x = X[0:trainStop, :]\n    train_y = y[0:trainStop]\n    val_x = X[trainStop:valStop, :]\n    val_y = y[trainStop:valStop]\n    test_x = X[valStop:, :]\n    test_y = y[valStop:]\n    return train_x, train_y, val_x, val_y, test_x, test_y\n\nif __name__ == \"__main__\":\n    X, y = parseData(\"wdbc.data\")\n    y = numpy.array(y)\n    Xs = X[:, [6, 20, 26]]\n    #Xs = X[:, [1, 4, 6, 8, 10, 11, 14, 15, 18, 20, 21, 28, 29]]\n    #Xs = X\n    Xs, y, val_x, val_y, test_x, test_y = splitData(Xs, y, 0.7, 0.15, 0.15)\n\n    #w, b = train(Xs, y, 50, 0.01, 1000)\n    #w, b = train(Xs, y, 20, 0.05, 1000)\n    #w, b = train(Xs, y, 96, 0.26673740016620473, 2207)\n    #w, b = train(Xs, y, 97, 0.25115243547839416, 975)\n    #w, b = train(Xs, y, 5, 1.0, 1612)\n    #printAccuracy(test_x, test_y, w, b)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 20, 0.05, 1000, 5)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 96, 0.26673740016620473, 2207, 13)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 25, 0.2939820078373507, 2426, 25)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 17, 0.14644249958668085, 2375, 25)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 5, 0.8560886757562707, 2500, 8)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 5, 0.001, 1894, 22)\n    #atrains, atests, awholes, wks, bks = CV_kfold(Xs, y, 250, 0.001, 2295, 2) #all features\n    #print_kfold_accuracy(atrains, atests, awholes)\n    #printCVtestAccuracy(test_x, test_y, wks, bks)\n    #printCVAllAccuracy(Xs, y, val_x, val_y, test_x, test_y, wks, bks)\n\n\n    space = [\n        Integer(5, 250, name = 'batchSize'),\n        Real(0.001, 1, name = 'learningRate'),\n        Integer(50, 2500, name = 'iterations')\n    ]\n    space2 = [\n        Integer(5, 250, name = 'batchSize'),\n        Real(0.001, 1, name = 'learningRate'),\n        Integer(50, 2500, name = 'iterations'),\n        Integer(2, 25, name = 'k')\n    ]\n    #optimal = gp_minimize(objective, space, n_calls = 20)\n    #print(optimal)\n    #optimal2 = gp_minimize(objective4D, space2, n_calls = 50)\n    #print(optimal2)\n\n    #infog = information_gain(X, y, \"#4DBEEE\", \"info_gain_bar.png\")\n    #fishers = fishers_score(X, y, \"#E50000\", \"fisher_scores_bar.png\")\n    #correlation_coefficient(X, y, \"correlation_heatmap.png\")\n    #dispersion_ratio(X, \"#4DBEEE\", \"dispersion_ratio_bar.png\")\n    #superimpose_bar(infog, fishers, \"#4DBEEE\", \"#E50000\", \"super_imposed_bar.png\")\n    #weighted_sum_importance(infog, fishers, \"#9A0EEA\", \"weighted_sum_bar.png\")\n    #btraina, btesta, boa, btrainai, btestai, boai = grid_best_performance_bs(Xs, y, 20, 0.05, 1000, 5)\n    #btraina, btesta, boa, btrainai, btestai, boai = grid_best_performance_lr(Xs, y, 20, 0.05, 1000, 5)\n    #btraina, btesta, boa, btrainai, btestai, boai = grid_best_performance_it(Xs, y, 20, 0.05, 1000, 5)\n    #btraina, btesta, boa, btrainai, btestai, boai = grid_best_performance_k(Xs, y, 96, 0.26673740016620473, 2207, 25)\n    #print(btraina)\n    #print(btesta)\n    #print(boa)\n    #print(btrainai)\n    #print(btestai)\n    #print(boai)\n\n    #plot_data_2d(Xs, y, \"2d_data.png\")\n    #plot_2d_boundary(Xs, y, w, b, \"2d_boundary.png\")\n    #plot_data_3d(Xs, y, \"3d_data.png\")\n    #plot_3d_boundary(Xs, y, w, b, \"3d_boundary.png\")","metadata":{"cell_id":"9f92923790ae43e180d55428d75c16d6","source_hash":"1d4e49eb","execution_start":1683072888311,"execution_millis":2138,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6aa49324-e0d5-405c-b842-af67c1bb30bb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"99d03d53fabd41dfb50024a5e72c9de8","deepnote_persisted_session":{"createdAt":"2023-05-03T00:31:19.877Z"},"deepnote_execution_queue":[]}}